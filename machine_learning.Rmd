---
title: "Machine Learning"
author: "Erin Anthony"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# once I have the correct cutoff value create the lr model on the test data
```

### Machine learning summary

The machine learning problem that I am exploring in this analysis is how to predict the proportion of minority enrollments at an institution of higher education using various institutional characteristics as the predictor variables. This is a supervised problem, because both predictor and target variable values are available in the data set and an algorithm will be created to map the input variables onto the output variables. I will also convert this from a regression to a classification problem by converting the target variable from a numerical variable to a categorical variable with three levels: low, medium, and high.

The independent variables that will be used in the initial model are location region, HBCU status, tribal college status, degree of location urbanization, open enrollment status, land grant status, Associate's degree offering, all-distance courses, some distance courses, study abroad offering, weekend courses, remedial courses, counseling access, day care access, on-campus housing, meal plan offering, cost of in-state tuition, size, student-faculty ratio, average grant aid, averag loan aid, employment/placement services, and access to tuition payment options. The dependent variable is the categorical level of minority enrollment (defined as African-American and Hispanic/Latino enrollment) at the institution. 

The machine learning technique used to build a model will be a decision tree. The percentage of accurate predictions will be calculated to determine the model accuracy. 

### Convert minority enrollment to a categorial variable
```{r convert minority enrollment to a categorical variable}
median(edu.df$minorityEnroll) # 19%
edu.df$minorityEnroll <- with(edu.df, ifelse(minorityEnroll<=19, 0, 1))
table(edu.df$minorityEnroll) # 1131 low (0), 1089 high (1)
```

# Split the data into a training set and a testing set
```{r split the data into a training set and a testing set}
set.seed(500)
split = sample.split(edu.df$minorityEnroll, SplitRatio = 0.7)
train = subset(edu.df, split == TRUE)
test = subset(edu.df, split == FALSE)
```

# Create and tune a logistic regression model 
```{r create and tune a logistic regression model}
# create and examine a logistic regression model using all variables
lrModel = glm(minorityEnroll ~ ., data=train, family=binomial)
summary(lrModel)

# use logistic regression model to predict the target variable
predTrain = predict(lrModel, type="response")
summary(predTrain)
tapply(predTrain, train$minorityEnroll, mean) # 33% prob low, 66% prob high

# use stepwise selection to select more specific variables for the model
library(MASS)
step <- stepAIC(lrModel, direction="both")
step$anova # provides a final recommended model (below)

# create and examine the updated logistic regression model
lrModel2 = glm(minorityEnroll ~ region + hbcu + tribal + urban + landGrant + partDistance + studyAbroad + weekend + remedial + mealPlan + facultyRatio + loanAid + placeEmploy + tuitionOptions, data=train, family=binomial)
summary(lrModel2)

# use tuned logistic regression model to make predictions on the training data
predTrain2 = predict(lrModel2, type="response")
summary(predTrain2)
tapply(predTrain2, train$minorityEnroll, mean) # 33% prob low, 66% prob high

# plot the ROC curve for the model and the training data
predTrain = predict(lrModel2, type="response")
ROCRpred = prediction(predTrain, train$minorityEnroll)
ROCRperf = performance(ROCRpred, "tpr", "fpr")
plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,0.1), text.adj=c(-0.2,1.7))

# calculate AUC as a quality metric and select the best threshold value
auc = as.numeric(performance(ROCRpred, "auc")@y.values)
thresh = as.numeric(performance(ROCRpred, "auc")@x.values)
# 0.8319 accuracy rate per the area under the ROC curve
Rperf = performance(ROCRpred, "auc")
max = which.max(slot(Rperf, "y.values")[[1]])
acc = slot(Rperf, "y.values")[[1]][max]
cut = slot(Rperf, "x.values")[[1]][max]

# plot the ROC curve for the model and the test data
predTest = predict(lrModel2, type="response", newdata=test)
ROCRpred2 = prediction(predTest, test$minorityEnroll)
ROCRperf2 = performance(ROCRpred2, "tpr", "fpr")
plot(ROCRperf2, colorize=TRUE, print.cutoffs.at=seq(0,1,0.1), text.adj=c(-0.2,1.7))

# calculate AUC as a quality metric
auc = as.numeric(performance(ROCRpred2, "auc")@y.values)
# 0.7856 accuracy rate per the area under the ROC curve
```

### Create and tune a decision tree model 
```{r create and tune a decision tree model}
library(caTools)
library(rpart)
library(rpart.plot)
set.seed(1000)

# create and examine the decision tree
decTree = rpart(minorityEnroll ~ ., data=train, method="class", control=rpart.control(minbucket=25))
plotcp(decTree)
prp(decTree)

# 4 variables used in the tree: region, grant aid, loan aid, in-state tuition

# use decision tree model to predict the target variable
prediction = predict(decTree, newdata=test, type="class")
table(test$minorityEnroll, prediction)
# (253 + 204) / (253 + 204 + 123 + 86) = 0.6862 accuracy rate

# plot the ROC curve of the prediction
predROC = predict(decTree, newdata = test)
pred = prediction(predROC[,2], test$minorityEnroll)
perf = performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE, print.cutoffs.at=seq(0,1,0.1), text.adj=c(-0.2,1.7))

# calculate AUC as a quality metric
predAUC = predict(decTree, newdata=test, type="class")
rocObj <- roc(test$minorityEnroll, as.vector(as.numeric(predAUC)))
auc(rocObj) # 0.6851 accuracy rate per the area under the ROC curve

# use k-fold cross-validation to determine the correct complexity parameter
library(caret)
library(e1071)
fitControl = trainControl(method="cv", number = 10)
cartGrid = expand.grid(.cp=(1:50)*0.01)
tr = train(minorityEnroll ~ ., data=train, method="rpart", trControl=fitControl, tuneGrid=cartGrid) # cp = 0.01
decTreeCV = rpart(minorityEnroll ~., method="class", data=train, control=rpart.control(cp=0.01))
predCV = predict(decTreeCV, newdata=test, type="class")
table(test$minorityEnroll, predCV)
# (251 + 213) / (251 + 213 + 114 + 88) = 0.6967 accuracy rate 
# (251 + 88) / (251 + 88 + 114 + 213) = 0.5090 baseline model comparison
bestTree = tr$finalModel
prp(bestTree) # this tree is different than the original tree, and uses the 5 independent variables loan aid, region, in-state tuition, meal plan, and degree of urbanization

# calculate AUC as a quality metric for the adjusted tree
predAUC2 = predict(decTreeCV, newdata = test, type="class")
rocObj2 <- roc(test$minorityEnroll, as.vector(as.numeric(predAUC2)))
auc(rocObj2) # 0.6959 accuracy rate per the area under the ROC curve
```

### Machine learning summary continued - evaluation of the models

The decision tree model accurately predicts the outcome variable about 70% of the time, whereas the logistic regression model accurately predicts the outcome variable about 78% of the time.